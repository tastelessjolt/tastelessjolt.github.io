# üìã Predictive Music Synthesis - Project Report

| **Status**      | Project Report                                                                           |
| --------------- | ---------------------------------------------------------------------------------------- |
| **Team**        | @Vishwajeet S 150050046 @Varshith S 150050084
@Sourabh S 150050009 @Harshith G 150050069 |
| **Description** | Recurrent neural networks to synthesize music of a particular genre                      |

# Introduction

We create a predictive model and train the model using music following a particular genre or style, or artist. Then synthesize parts of music using this model separately and stitch these parts (intro, verse, pre-chorus, chorus, bridge, outro) together in a musical sense.

 We anticipate this to be a music of the expected style. The main motivation behind this was to make ML models mimic an artist‚Äôs creativity and style of the genre. Our choice of genre is such that the music is periodic, with well defined structure and tones, doesn‚Äôt involve much complexity in terms of instruments and non-lyrical (basically as simple as possible) considering the primitive nature of the model, i.e., basic recurrent neural networks.
 
Genres under consideration included Indian Classical, Jazz, Western Classical (Beethoven, Bach, Mozart), EDM and finally chose Chopin. For our model we chose Musopen Chopin because it felt closest to our requirements.

# Datasets
## Voltage Approach
- Conversion of ‚Äò.ogg/.mp3‚Äô files to array of voltage levels (By converting to ‚Äò.wav‚Äô files and reading the music using scipy.io.wavfile module) and partitioning into batches to implement **Sliding Window** for prediction
- Chopin music in ‚Äòogg‚Äô format.
  - https://archive.org/details/musopen-chopin/
## *MIDI Approach

We converted Musopen Chopin‚Äôs Midi files to Piano Roll representation (Notes representation)

- Musopen Chopin in MIDI format (Primarily trained on)
  - [http://www.midiworld.com/chopin.htm](https://l.facebook.com/l.php?u=http%3A%2F%2Fwww.midiworld.com%2Fchopin.htm&h=ATNxqfLfxlzepi7zazSBAu4n_xrKl3xuaePbCyztxg1hkN-Q_jLAH5T2mwUM8GRnow7KJIHH7j4IMmUlEBmU_CtZ-LG9BK6JHQ-6v6RP2lGFNk08bO5bKmU8KJ0hkcZG)
- Bach chorales
  - http://www.jsbach.net/
# Core Idea
- Song is abstracted as a sequence of Data-points. The Data-point may be a voltage level or MIDI event or Piano roll event
- We fix a Input Size of Data-point for our neural network and train it to generate the next data point, then sliding this window by one over the sequence of Data-points 
- For Generation of Data-points we take similar approach. Fixing a seed as initial window giving the Neural Network a flavor to start the window.
- Predicting the next data point and then sliding the window one data point ahead again.


# Challenges
## Voltage
- Bulky nature of data set (wav files) due to high sampling rates of songs which causes major complication in training time
- Exploring the use of recurrent regression because of inconvenience for representing regression as a classification problem
  - http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html
- Conversion of Regression problem to a classification problem for RNN+LSTM to work. By using binary encoding of each digit as an output generated by the model


## *MIDI
- Meta messages
  - Midi files, which are encoded as ‚Äúmessages‚Äù signal events to various devices in a system that need to occur at various times.
  - These messages often include ‚Äúmeta‚Äù messages that don‚Äôt necessarily signal ‚Äúsound producing‚Äù messages, but are crucial to the song (meta messages include volume changes, instrument changes, pitch changes, etc).
  - These are encoded in a byte sequence of a different length as compared to the actual messages. Conversion of these messages into a common format, requires padding of bits.
  - Distinction among these after prediction often results in meaningless messages.


## *Piano Roll
- While converting MIDI to human understandable format also poses the problem of identifying them as meta (system or channel) messages from which information can be extracted
- To overcome this we switched to an equivalent representation called piano roll representation or note state matrix. This contains data points for 78 keys to played or not in a piano with constant time period for each that is of 120 beats per minute.
- The task was to convert MIDI file to note state matrix and train the model. After the training is done we convert them back to midi format 






## *Sequential Predictive Models
- Models involving sequential prediction (models that generate output based on output in a previous iteration) have to deal with propagation of undesired notes. This effect is especially pronounced in our model that uses three LSTM layers.
- One instance of our model generated a sequence of zeros
  
## *Analyzing Results
- The primary difficulty with analysing songs is that a metric to judge a song is extremely subjective. Differences in opinion lie in personal preferences.
- An analytical approach to judge music in any form requires domain knowledge, involving study of chord sequences, notes and other patterns, as performed in;
  - http://www.cse.nd.edu/Reports/2008/TR-2008-10.pdf 
# Implementation Methods & Libraries
## Voltage
  RNN and LSTM using TensorFlow
  - RNN+LSTM are supposed to be the best models while dealing with Temporal sequence
  - **Training**
    - The main idea is to train the model to take a input(sliding window) and generate the next voltage level. Then shifting the window forward we repeat the process hence generating temporal sequence which can be converted back to ‚Äò.wav‚Äò files  
  - **Synthesis**
    - The first input being a seed so that it can start of to generate the temporal sequence as mentioned above shifting window each each time it generates
## *MIDI (Piano Roll ‚Äî Notes)
  Used MIDO a python library to handle the parsing of MIDI files and also for converting MIDI files to Piano Roll i.e Matrix representation of the notes being played at a given moment.
  Used Keras a high-level neural networks API over Tensor Flow.
- Again used RNN+LSTM as they suitable for Temporal sequences
  - **Training**
    - Similar to that of Voltage training we used Sliding window.
    - Many models trained with varying window sizes and size of data set 
  - **Synthesis**
    - A seed is used for it initializing the generation of temporal sequence same as mentioned as above.
    - Then converting this Piano Roll to MIDI to be playable.





## Other Potential Implementaion
1. **Markov Chains**
  1. An easy way to start with the project but the result RNN+LSTM showed a greater potent
    1. [https://youtu.be/SacogDL_4JU](https://youtu.be/SacogDL_4JU)
2. **Adversarial** **Networks**
  1. It requires a lot less data set to train on, reducing the training time but convergence is not guaranteed
  2. And also the generation is better than RNN+LSTM
  3. But difficult for implementing and possibily beyond scope of this project‚Äôs time constraints
  4. They are trained such that they generate music not as we want temporal sequence



## Libraries
1. Keras and Tensor Flow creating the Neural Network
2. Scipy for handling wav files
3. Mido for managing midi files  
4. Numpy
5. Librosa


# Timeline
1. Briefly went through many research papers for guidance and ideas mentioned below
2. Settled with RNN LSTM as our model started Explored Tutorials for TensorFlow for implementing RNN+LSTM  models 
3. Agreed upon major Data Format
  1. Voltage levels from ‚Äú.wav‚Äù 
  2. Notes from ‚Äú.mid‚Äù
  3. Piano Rolls from ‚Äú.mid‚Äù
4. Explored use of MFCC using libraries like python speech features, librosa.
5. Developed a working implementation of the model using tensor flow trained over voltage levels from the .wav files
6. ----------------------------------------Mid Project Evaluation---------------------------------------------
7. Shifted to MIDI file format and explored MIDO (Python library) for reading and creating them
8. Tensor Flow became complicated, hence shifted to Keras. Developed a model using Keras
9. Felt the need to shift to Piano Rolls as encoding of MIDI events was getting quite complicated 
10. Implemented a model trained on piano rolls from midi files and generated few songs
11. Started working on performance metrics, to see how well the model is working
12. Worked on post-processing generated music to remove excess background noise








# Analysis and Performance Metrics
- We are employing performance metrics to compare the similarity between the song produced by us and the songs it was trained on. For that we converted our song to .wav format and used python module for audio analysis.
- We compute the tempograms, spectrograms, chromagrams and recurrence matrices. (Images attached)


| Song No | Parameters           | Comments                                | Tempograph                                                                                                    | Chromagraph                                             | Spectrogram                                         | Recurrence Matrix                       |
| ------- | -------------------- | --------------------------------------- | ------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------- | --------------------------------------------------- | --------------------------------------- |
| Ideal   | -                    | -                                       | Specific tempos at a specific time                                                                            | Non constant waves                                      | Variable and biased to lower pitches                | Observable lines across time            |
| 1       | s8, e10, i20, b50    | Not properly trained; No thresholding   | Tempos spread across high ranges                                                                              | Extremely periodic for each note                        | Too much repetition                                 | Short term patterns scattered across    |
| 2       | s8, e10, i20, b50    | Not properly trained; Good thresholding | Lesser spread and well defined but too periodic                                                               | DIstributed well                                        | Observable pattern but larger spread                | Shorter patterns but not scattered much |
| 3       | s8, e100, i50, b50   | Failed training                         | Bad                                                                                                           | Bad                                                     | Bad                                                 | Spread across time                      |
| 4       | s20, e100, i100, b50 | Trained for 20 min                      | Bad and biased towards higher ranges but certain tempos are not dominant. Might improve after port-processing | Patterns about some notes and spread across other notes | Good                                                | Well formed blocks and diagonals        |
| 5       | s10, e100, i20, b500 | Best result post-processing             | Sufficiently sparse                                                                                           | Certain classes periodic, while others are sparse       | Sufficient variation across time for each frequency | Not as structured as others             |

s = #files, e = #epochs, i = windowsize, b = batch_size





# Research Papers and References
- http://cs229.stanford.edu/proj2012/Victor-JazzMelodyGenerationAndRecognition.pdf
  Jazz generation is simple because of markov chains but recognition is harder which is done by naive bayes (all points are independent) and hidden markov models.
- http://www.cse.nd.edu/Reports/2008/TR-2008-10.pdf
  Generating Bach melodies by first training it and generating a set of rules on it‚Äôs own and then generating next set of points from a given piece of music. 
- https://arxiv.org/pdf/1206.6392v1.pdf
  Uses RNN based neural network to generate really impressive music.
- [http://people.idsia.ch/~juergen/blues/IDSIA-07-02.pdf](https://l.messenger.com/l.php?u=http%3A%2F%2Fpeople.idsia.ch%2F~juergen%2Fblues%2FIDSIA-07-02.pdf&h=ATNcsHfpv5brpG_PxDLo4dpO-f60aJSUPb_9LvCSlf7grk71M7SSpeNSURe0tDTnhej_If7-xC375kB3LA2HOw_jJFWhZ9usxNgg6_f6x2LHGs0NUv4HtvDLanQGNv2n)
  The paper uses LSTM-RNN based model to generate blues music. 
- http://www2.cmpe.boun.edu.tr/courses/cmpe362/spring2014/files/projects/MFCC%20Feature%20Extraction.pdf
  MFCC Analysis
- https://bmcfee.github.io/papers/ismir2014_spectral.pdf
  Analysis and metrics for the songs
## Others
- https://www.quora.com/Music-How-can-machine-learning-be-used-to-analyze-music
- http://www.datasciencecentral.com/profiles/blogs/using-machine-learning-to-generate-music
- https://livingthing.danmackinlay.name/generative_art_neural_networks.html
- http://www.asimovinstitute.org/analyzing-deep-learning-tools-music/
- http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/

*Post mid-evaluation

